# Task -3
# LGMVIP---DataScience-Global-terrorisM

This project is based on the Exploratory Data Analysis of Global Terrorism. I have used various kinds of basic plots like Matplotlib and Seaborn to give an initial look of the data.

# Project Title : Global Terrorism 

This repository presents Global Terrorism Dataset Exploratory data analysis.
With the help of this project we can see some useful insights and make decisions.

# Introduction

The scope of this project is to drill down the terrorist events around the world from 1970 through 2015

The primary objectives are...

To identify and highlight the geographical and temporal patterns of the terrorism,
To discover the main parameters of a successful terrorist attack, and
To allow the user to customize the analysis and to explore the data in the most interactive way.
** The idea behind the project is to find out how the terrorism has developed in the Western world and whether we need to build tall walls to protect ourself against future threats. We chose our topic to be more global oriented, because

It enables aggregation on many geographical levels including the globe, regions, countries, states, and cities It is very diversified and encapsulates many interesting attributes It has both temporal and geographical data

# Project Goal
This project is based on Global Terrorism Exploratory Data Analysis to get insights about the Most attacked country, region , year and many more things** The data here i used is taken from LGMVIP Internship and the data is from since 1970 to 2015.

# ‚è≥ Dataset
The dataset is very comprehensive and contains a lot of terrorism-related information. We downloaded the entire dataset Global Terrorism Database, available from LGMVIP Internship Task. It contains 1,81,691 terrorist attacks x 135 features, and takes 187.1+ MB of disk space. It's worth to mention that it is almost completely encoded (strings/long numbers to short numbers). To decode the dataset we looked at the codebook available here. After exploring the codebook we discovered some columns to be redundant, or not relevant, which we removed. See the corresponding notebook Cleaning Data for further details on how we approached.

We ended up work on 19 columns, which contain the quantitative as well as the qualitative information of the main interest. After decoding, cleaning, filtering, and encoding steps, we've got 46,556 rows x 23 columns, or equivalently 7.1+ MB of disk space.

# Author
Harshitha-Chalumuri

# Libraries used
Matplotlib 
Pandas  
Numpy  
Seaborn 



GLOBALTERRORISM TASK 3
DATASET LINK https://drive.google.com/file/d/1dsBl7RkT-8odto2BrXFe-oqTbpgRQQhk/view?usp=share_link
